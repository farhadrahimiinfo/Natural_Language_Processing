{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4oi2XBUrDje"
      },
      "source": [
        "\n",
        "# Introduction to Text Analysis with Python\n",
        "\n",
        "Welcome to the Digital Scholarship Lab introduction to Text Analysis with Python class. In this class we'll learn the basics of text analysis:\n",
        "\n",
        "- parsing text\n",
        "- analyzing the text\n",
        "\n",
        "We'll use our own home made analysis tool first, then we'll use a python library called `TextBlob` to use some built-in analysis tools.\n",
        "\n",
        "This workshop assumes you've completed our Intro to Python [workshop](https://brockdsl.github.io/Intro_to_Python_Workshop/)\n",
        "\n",
        "\n",
        "\n",
        "Be sure to enable line numbers by looking for the 'gear' icon and checking the box in the 'Editor' panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGHjZYBArDjg"
      },
      "source": [
        "## EG. Scrabble!\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5d/Scrabble_game_in_progress.jpg\" width =500x>\n",
        "\n",
        "Scrabble is a popular game where players try to score points by spelling words and placing them on the game board. We'll use Scrabble scoring our our first attempt at text analysis. This will demonstart the basics of how Text Analysis works.\n",
        "\n",
        "The function below gives you the Scrabble scored of any word you give it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "FVS8oeZjrDjh"
      },
      "outputs": [],
      "source": [
        "# This function will return the Scrabble score of a word\n",
        "\n",
        "def scrabble_score(word):\n",
        "\n",
        "    #Dictionary of our scrabble scores\n",
        "    score_lookup = {\n",
        "        \"a\": 1,\n",
        "        \"b\": 3,\n",
        "        \"c\": 3,\n",
        "        \"d\": 2,\n",
        "        \"e\": 1,\n",
        "        \"f\": 4,\n",
        "        \"g\": 2,\n",
        "        \"h\": 4,\n",
        "        \"i\": 1,\n",
        "        \"j\": 8,\n",
        "        \"k\": 5,\n",
        "        \"l\": 1,\n",
        "        \"m\": 3,\n",
        "        \"n\": 1,\n",
        "        \"o\": 1,\n",
        "        \"p\": 3,\n",
        "        \"q\": 10,\n",
        "        \"r\": 1,\n",
        "        \"s\": 1,\n",
        "        \"t\": 1,\n",
        "        \"u\": 1,\n",
        "        \"v\": 4,\n",
        "        \"w\": 4,\n",
        "        \"x\": 8,\n",
        "        \"y\": 4,\n",
        "        \"z\": 10,\n",
        "        \"\\n\": 0, #just in case a new line character jumps in here\n",
        "        \" \":0 #normally single words don't have spaces but we'll put this here just in case\n",
        "\n",
        "    }\n",
        "\n",
        "    total_score = 0\n",
        "\n",
        "    #We look up each letter in the scoring dictionary and add it to a running total\n",
        "    #to make our dictionary shorter we are just using lowercase letters so we need to\n",
        "    #change all of our input to lowercase with .lower()\n",
        "    for letter in word:\n",
        "        total_score = total_score + score_lookup[letter.lower()]\n",
        "\n",
        "    return total_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq8VnePYrDjk"
      },
      "source": [
        "Text Analysis is a process comprised of three basic steps:\n",
        "1. Identifying the text (or corpus) that you'd like to an analyze\n",
        "1. Apply the analysis to your prepared text\n",
        "1. Review the results\n",
        "\n",
        "In our very basic example of scrabble we just are interested in finding the points we would get for spelling a specific word.\n",
        "\n",
        "In a more complex example with a larger corpus you can do any of the following types of analysis:\n",
        "- determine the sentiment (positive / negative tone) of the text\n",
        "- quantify how complex a piece of writing is based on the vocabulary it uses\n",
        "- determine what topics are in your corpus\n",
        "- classify your text into different categories based on what it is about\n",
        "\n",
        "Of course, there are many other different outcomes you can get from peforming text analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5JmHik_rDjl"
      },
      "source": [
        "Try questions Q1 - Q2 and type \"All Done\" in the chat box when you are done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHWs7sSzrDjl"
      },
      "source": [
        "## Q1\n",
        "\n",
        "Score your name by creating the text variable _name_ on line 1.\n",
        "\n",
        "How many Points do you get for your name? Complete the expression below to find out the scrabble score of your name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CGDSzYr6rDjl"
      },
      "outputs": [],
      "source": [
        "name = \"\"\n",
        "print(\"Score for my name is:\", scrabble_score(name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy5kTjNcrDjm"
      },
      "source": [
        "## Q2\n",
        "\n",
        "Score your pet's name (or favorite character from a story)  by creating the text variable _pet_name_ on line 1.\n",
        "Does your name or the name of your pet score higher in Scrabble?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yu8AeDXVrDjn"
      },
      "outputs": [],
      "source": [
        "pet_name = \"\"\n",
        "print(\"Score for my pet's name is:\",scrabble_score(pet_name))\n",
        "\n",
        "#Compare to see which gets more points!\n",
        "if scrabble_score(pet_name) > scrabble_score(name):\n",
        "    print(\"My pet's name scores more points!\")\n",
        "else:\n",
        "    print(\"My name scores more (or the same) amount of points as my pets name\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZveM5cLrDjo"
      },
      "source": [
        "# Beyond the basics\n",
        "\n",
        "We just completed a very basic text analysis where we analyzed two different bits of text to see which one scores higher in Scrabble. Let's expand this idea to a more complex example using the [TextBlob](https://textblob.readthedocs.io/en/dev/) Python Library. There are other more complex libraries that you can use for text analysis, we are using more simple solutions so we can spend more time looking at results compared to setting up the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mJSLeUrDjo"
      },
      "source": [
        "# Installing and Loading the Libraries\n",
        "\n",
        "This next cell will install and load the requires libraries that will do the text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CaGM6yKYrDjo"
      },
      "outputs": [],
      "source": [
        "#Install textblob using magic commands\n",
        "#Only needed once\n",
        "%pip install textblob\n",
        "#%python -m textblob.download_corpora\n",
        "#%pip install textblob.download_corpora\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#Let's make sure our previews show more information\n",
        "pd.set_option('display.max_colwidth', 999)\n",
        "\n",
        "#Classifier for laster\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob import Word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D7EbzdRrDjp"
      },
      "source": [
        "# Corpus\n",
        "\n",
        "![winnie_splash](https://raw.githubusercontent.com/BrockDSL/Text_Analysis_with_Python/master/winnie_splash.png)\n",
        "\n",
        "Corpus is a fancy way of saying the text that we will be looking at. Cleaning up a corpus and getting it ready for analysis is a big part of the process, once that is done the rest is easy. For our example we are going to be looking at some entries from the 1900 [diary](https://dr.library.brocku.ca/handle/10464/7282) of Winnie Beam. The next cell will load this corpus into a Pandas dataframe and show us a few entires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fet3LzyxrDjp"
      },
      "outputs": [],
      "source": [
        "winnie_corpus = pd.read_csv('https://raw.githubusercontent.com/BrockDSL/Text_Analysis_with_Python/master/winnie_corpus.txt', header = None, delimiter=\"\\t\")\n",
        "winnie_corpus.columns = [\"page\",\"date\",\"entry\"]\n",
        "winnie_corpus['date'] = pd.to_datetime(winnie_corpus['date'])\n",
        "winnie_corpus['entry'] = winnie_corpus.entry.astype(str)\n",
        "\n",
        "#preview our top entries\n",
        "winnie_corpus.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CLXqec9rDjp"
      },
      "source": [
        "# Measuring Sentiment\n",
        "\n",
        "We can analyze the _sentiment_ of the text (more [details](https://planspace.org/20150607-textblob_sentiment/).) The next cell demonstrates this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kIfdK-abrDjq"
      },
      "outputs": [],
      "source": [
        "\n",
        "happy_sentence = \"Python is the best programming language ever!\"\n",
        "sad_sentence = \"Python is difficult to use, and very frustrating\"\n",
        "\n",
        "\n",
        "print(\"Sentiment of happy sentence \", TextBlob(happy_sentence).sentiment)\n",
        "print(\"Sentiment of sad sentence \", TextBlob(sad_sentence).sentiment)\n",
        "\n",
        "# polarity ranges from -1 to 1.\n",
        "# subjectvity ranges from 0 to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Y6syyyrDjq"
      },
      "source": [
        "## Q3\n",
        "\n",
        "Try a couple of different sentences in the code cell below. See if you can create something that scores -1 and another that scores 1 for _polarity_. See if you can minimize the _subjectivity_ of your sentence. *Share your answers in the chat box*.\n",
        "\n",
        "(We can create a multi line string of text by putting it in triple quotes like the cell following.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "gd7ba7A2rDjq"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "print(\"Score of test sentence is \", TextBlob(test_sentence).sentiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZqS_-klrDjr"
      },
      "source": [
        "# Adding Sentiment to our Diary entries\n",
        "\n",
        "This next cell will score each diary entry in a new column that will be added to the dataframe. We loop through each entry, calculate the two scores that represent the sentiment. After all the scores are computed with add them to the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KcjVfXErrDjr"
      },
      "outputs": [],
      "source": [
        "#Apply sentiment analysis from TextBlob\n",
        "\n",
        "polarity = []\n",
        "subjectivity = []\n",
        "\n",
        "\n",
        "for day in winnie_corpus.entry:\n",
        "    #print(day,\"\\n\")\n",
        "    score = TextBlob(day)\n",
        "    polarity.append(score.sentiment.polarity)\n",
        "    subjectivity.append(score.sentiment.subjectivity)\n",
        "\n",
        "winnie_corpus['polarity'] = polarity\n",
        "winnie_corpus['subjectivity'] = subjectivity\n",
        "\n",
        "\n",
        "#Let's look at our new top entries\n",
        "winnie_corpus.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMVg4H_8rDjr"
      },
      "source": [
        "Now that we have daily sentiment values, let's try to visualize how they go up and down over the course of the first 3 months of the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "D2pNcEaGrDjr"
      },
      "outputs": [],
      "source": [
        "#Let's graph out the sentiment as it changes day to day.\n",
        "\n",
        "plt.plot(winnie_corpus[\"date\"],winnie_corpus[\"polarity\"])\n",
        "plt.xticks(rotation='45')\n",
        "plt.title(\"Sentiment of Winnie's Diary Entries\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN65nXHrDjr"
      },
      "source": [
        "## Interesting spikes?\n",
        "\n",
        "We see some really strong negative and positive spikes in the sentiment. Let's just take a look at some of those entries. Run the next three cells to look at the individual negative and positive entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_99o-r_GrDjr"
      },
      "outputs": [],
      "source": [
        "#instead of looking at just the hightest and lowest value we'll reduce that number by a threshold value\n",
        "#that way we can see numbers that are close to the highest sentiment and the lowest sentiment\n",
        "#we'll start with 20%.\n",
        "\n",
        "\n",
        "threshold = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bteicS5BrDjr"
      },
      "outputs": [],
      "source": [
        "#Very Negative\n",
        "bad_sentiment = winnie_corpus[\"polarity\"].min()\n",
        "\n",
        "#Reduce this number by threshold %\n",
        "bad_sentiment = bad_sentiment - (bad_sentiment * threshold)\n",
        "\n",
        "winnie_corpus[winnie_corpus[\"polarity\"] <= bad_sentiment]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUONrRtorDjs"
      },
      "outputs": [],
      "source": [
        "#Very Positive\n",
        "good_sentiment = winnie_corpus[\"polarity\"].max()\n",
        "\n",
        "#Reduce this number by threshold %\n",
        "good_sentiment = good_sentiment - (good_sentiment * threshold)\n",
        "\n",
        "winnie_corpus[winnie_corpus[\"polarity\"] >= good_sentiment]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne_WKq1qrDjs"
      },
      "source": [
        "## Q4\n",
        "\n",
        "Do you agree with the sentiment scores that are applied in the above two cells? Share your thoughts in the chat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ly5npu0rDjs"
      },
      "source": [
        "# What else can we get from the text?\n",
        "\n",
        "We've seen some details about sentiment, but what else can we get from the text? Let's grab a random entry and see what we can find out about it. We'll choose the *22*nd entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "xZr9oqdErDjs"
      },
      "outputs": [],
      "source": [
        "entry_number = 22\n",
        "bit_of_corpus = TextBlob(winnie_corpus[\"entry\"][entry_number])\n",
        "bit_of_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZq1QVVjrDjt"
      },
      "source": [
        "# Sentences and Sentiment\n",
        "\n",
        "We applied sentiment on to daily entries but we can apply it down to sentences just to see how a score fluctuates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6sW1h53hrDjt"
      },
      "outputs": [],
      "source": [
        "for sentence in bit_of_corpus.sentences:\n",
        "    print(sentence)\n",
        "    print(sentence.sentiment,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmUNuBo3rDjt"
      },
      "source": [
        "# Words in sentences\n",
        "\n",
        "You can parse through words in a sentence using TextBlob as well. The next cell illustrates this. We'll need to to get to calculate specific sentiment scores in our next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qvS_CujjrDjt"
      },
      "outputs": [],
      "source": [
        "for sentence in bit_of_corpus.sentences:\n",
        "    for word in sentence.words:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jmqrNlIrDjt"
      },
      "source": [
        "## Q5\n",
        "\n",
        "Another random journal entry. Pick a random number between 1 and the length of the dataframe and update *en_no* in line 1. If you get an interesting result, share it with the class in the chat box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_K1IqPRRrDjv"
      },
      "outputs": [],
      "source": [
        "#Pick a value between 1 and this number\n",
        "len(winnie_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PcWVLTDurDjv"
      },
      "outputs": [],
      "source": [
        "en_no =\n",
        "\n",
        "another_bit_of_corpus = TextBlob(winnie_corpus[\"entry\"][en_no])\n",
        "\n",
        "print(\"Random Entry: \\n\")\n",
        "print(another_bit_of_corpus,\"\\n\")\n",
        "\n",
        "#Go through all of the sentences of this entry and determine their sentiment\n",
        "for sentence in another_bit_of_corpus.sentences:\n",
        "    print(sentence)\n",
        "    print(sentence.sentiment,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgCUyaTprDjv"
      },
      "source": [
        "# Stopwords\n",
        "\n",
        "Often we want to remove common words (eg. a, the, of) in our corpus before we analyze things. For the most part TextBlob will ignore these words if we use the right analysis. We'll just look at it here to understand the idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHHRrs_DrDjv"
      },
      "outputs": [],
      "source": [
        "for word in stopwords.words('english'):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sdSZXfirDjv"
      },
      "source": [
        "Now let's see how we can remove stopwords from a piece of text. Experiment by changing to a different example sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ_Po2EKrDjw"
      },
      "outputs": [],
      "source": [
        "ex_text = TextBlob(\"I know this, do you?\")\n",
        "\n",
        "\n",
        "for sentence in ex_text.sentences:\n",
        "    for word in sentence.words:\n",
        "\n",
        "        if word.lower() not in stopwords.words('english'):\n",
        "            print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UECmD6qrDjw"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Some times analysis requires us to **stem** or **lemmatize** our word so that is becomes a root word. For example the word _cats_ would be transformed into _cat_. Doing this makes our analysis a bit more clear as we can more readily compare words against one another. Our Python Library allows us to do this fairly easily.\n",
        "\n",
        "A full & comprehensive text analysis project would require us to do this for our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPunul4RrDjw"
      },
      "outputs": [],
      "source": [
        "w = Word(\"cats\")\n",
        "w.lemmatize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTeOXb3rDjw"
      },
      "outputs": [],
      "source": [
        "ex_text_2 = TextBlob(\"I have chased so many mice and cats today, I am exhausted.\")\n",
        "\n",
        "\n",
        "for sentence in ex_text_2.sentences:\n",
        "    for word in sentence.words:\n",
        "        print(Word(word).lemmatize())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKZRy9NrDjx"
      },
      "source": [
        "## Sentiment v. Stop Words v. Lemmatization\n",
        "\n",
        "The question now becomes does Sentiment score change now if we apply our processing steps? Let's try by taking our random entry and applying these steps and getting the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quRdpMV1rDjx"
      },
      "outputs": [],
      "source": [
        "#Sentiment of unchanged entry\n",
        "raw_sentiment = another_bit_of_corpus.sentiment\n",
        "\n",
        "print(\"\\nSentiment of entry: \\n\",raw_sentiment)\n",
        "\n",
        "\n",
        "#Remove stopwords & score sentiment\n",
        "stopword_sent = \"\"\n",
        "for sentence in bit_of_corpus.sentences:\n",
        "    for word in sentence.words:\n",
        "        if word.lower() not in stopwords.words('english'):\n",
        "            stopword_sent = stopword_sent + \" \" + str(word)\n",
        "\n",
        "stopword_sentiment = TextBlob(stopword_sent).sentiment\n",
        "print(\"\\nSentiment of entry without stopwords: \\n\",stopword_sentiment)\n",
        "\n",
        "\n",
        "#Lemmatize the words and print sentiment\n",
        "lemm_sent = \"\"\n",
        "for sentence in bit_of_corpus.sentences:\n",
        "    for word in sentence.words:\n",
        "        lw = str(Word(word).lemmatize())\n",
        "        lemm_sent = lemm_sent + \" \" + lw\n",
        "\n",
        "lemm_sentiment = TextBlob(lemm_sent).sentiment\n",
        "print(\"\\nSentiment of entry with lemmatization: \\n\",lemm_sentiment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQZi3IUOrDjx"
      },
      "source": [
        "---\n",
        "Bottom line, our sentiment tool already considers stopwords and lemmatization. In *this case* we didn't have to process the text further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NWSLk5orDjx"
      },
      "source": [
        "# Noun Phrases\n",
        "\n",
        "We can get a good idea about what a corpus is about by looking at the different _nouns_ that show up in it. _Nouns_ that show up a lot give us an idea of the contents of the text. TextBlob is smart enough to ignore *stopwords* when it does this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eBg8HTZErDjx"
      },
      "outputs": [],
      "source": [
        "for np in bit_of_corpus.noun_phrases:\n",
        "    print(np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eme-EPxcrDjx"
      },
      "source": [
        "### Automatic Keyword generator\n",
        "\n",
        "One good use of Noun Phrase identification is automatically creating keywords for a collection of works in your corpus. The basics structure goes like this:\n",
        "\n",
        "1. Read through each document in your corpus\n",
        "\n",
        "2. Identify each noun phrase in your documents\n",
        "\n",
        "3. NP that show up the most are the keywords for your document\n",
        "\n",
        "We are going to be looking at the book [The Prince](https://en.wikipedia.org/wiki/The_Prince) (You can modify line #4 to download a different book, just pick the full-text [Guttenberg](https://www.gutenberg.org/) URL for the variable.)\n",
        "\n",
        "You'll need to be patient while this cell runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTtNyzPfrDjx"
      },
      "outputs": [],
      "source": [
        "keywords = dict()\n",
        "\n",
        "#You can replace with any book on Gutenberg, we are using The Prince - https://www.gutenberg.org/ebooks/1232\n",
        "BOOK_URL = \"https://www.gutenberg.org/files/1232/1232-0.txt\"\n",
        "\n",
        "\n",
        "#We are using a Library called requests to download the book (https://realpython.com/python-requests/)\n",
        "print(\"Downloading book...\")\n",
        "book = requests.get(BOOK_URL)\n",
        "\n",
        "#Turn text into text blob\n",
        "book_blob = TextBlob(book.text)\n",
        "\n",
        "\n",
        "print(\"Identiying Noun phrases and building frequency dictionary...\")\n",
        "\n",
        "#Go through all noun phrases\n",
        "for np in book_blob.noun_phrases:\n",
        "    if np in keywords:\n",
        "        keywords[np] += 1\n",
        "    else:\n",
        "        keywords[np] = 1\n",
        "\n",
        "\n",
        "#Sort dictionary and print top 20 entries\n",
        "print(\"Most common Nouns...\")\n",
        "\n",
        "for np in sorted(keywords, key=keywords.get, reverse=True)[0:20]:\n",
        "    print(np, keywords[np])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG3RTihErDjy"
      },
      "source": [
        "## A closer look at the corpus\n",
        "\n",
        "Let's look at the January Diary entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Zwv28xesrDjy"
      },
      "outputs": [],
      "source": [
        "#January Entries\n",
        "jan_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-01-01') & (winnie_corpus['date'] <= '1900-01-31')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6H9MjKZrDjy"
      },
      "source": [
        "Let's see what Winnie talks about the most in the month. We can do this by extracting the _noun phrases_ in her entries. We can put them in a dictionary to count how many times a phrase is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jaCrC0bQrDjz"
      },
      "outputs": [],
      "source": [
        "jan_phrases = dict()\n",
        "\n",
        "for entry in jan_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in jan_phrases:\n",
        "            jan_phrases[np] += 1\n",
        "        else:\n",
        "            jan_phrases[np] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pjhToZL1rDjz"
      },
      "outputs": [],
      "source": [
        "#Print the top 10 things she mentioned in January\n",
        "\n",
        "for np in sorted(jan_phrases, key=jan_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, jan_phrases[np])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wstWnlWirDjz"
      },
      "source": [
        "## Q6\n",
        "\n",
        "Let's compare against the first 6 months of the year. Run the following set of cells.\n",
        "What can you say about Winnie's topics over the first half of the year? Share your thoughts in the chat box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bFbpld55rDjz"
      },
      "outputs": [],
      "source": [
        "#February Entries\n",
        "feb_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-02-01') & (winnie_corpus['date'] <= '1900-02-28')]\n",
        "\n",
        "feb_phrases = dict()\n",
        "\n",
        "for entry in feb_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in feb_phrases:\n",
        "            feb_phrases[np] += 1\n",
        "        else:\n",
        "            feb_phrases[np] = 1\n",
        "\n",
        "#Print the top 10 things she mentioned in February\n",
        "\n",
        "for np in sorted(feb_phrases, key=feb_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, feb_phrases[np])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "kBvwN1PJrDjz"
      },
      "outputs": [],
      "source": [
        "#March Entries\n",
        "mar_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-03-01') & (winnie_corpus['date'] <= '1900-03-31')]\n",
        "\n",
        "\n",
        "mar_phrases = dict()\n",
        "\n",
        "for entry in mar_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in mar_phrases:\n",
        "            mar_phrases[np] += 1\n",
        "        else:\n",
        "            mar_phrases[np] = 1\n",
        "\n",
        "#Print the top 10 things she mentioned in March\n",
        "\n",
        "for np in sorted(mar_phrases, key=mar_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, mar_phrases[np])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy-JcZwDrDj0"
      },
      "outputs": [],
      "source": [
        "#April Entries\n",
        "april_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-04-01') & (winnie_corpus['date'] <= '1900-04-30')]\n",
        "\n",
        "april_phrases = dict()\n",
        "\n",
        "for entry in april_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in april_phrases:\n",
        "            april_phrases[np] += 1\n",
        "        else:\n",
        "            april_phrases[np] = 1\n",
        "\n",
        "#Print the top 10 things she mentioned in April\n",
        "\n",
        "for np in sorted(april_phrases, key=april_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, april_phrases[np])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R5tt1P3rDj0"
      },
      "outputs": [],
      "source": [
        "#May Entries\n",
        "may_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-05-01') & (winnie_corpus['date'] <= '1900-05-31')]\n",
        "\n",
        "may_phrases = dict()\n",
        "\n",
        "for entry in may_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in may_phrases:\n",
        "            may_phrases[np] += 1\n",
        "        else:\n",
        "            may_phrases[np] = 1\n",
        "\n",
        "#Print the top 10 things she mentioned in may\n",
        "\n",
        "for np in sorted(may_phrases, key=may_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, may_phrases[np])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuk5AkeVrDj0"
      },
      "outputs": [],
      "source": [
        "#June Entries\n",
        "june_corpus = winnie_corpus[(winnie_corpus['date'] >= '1900-06-01') & (winnie_corpus['date'] <= '1900-06-30')]\n",
        "\n",
        "june_phrases = dict()\n",
        "\n",
        "for entry in june_corpus.entry:\n",
        "    tb = TextBlob(entry)\n",
        "    for np in tb.noun_phrases:\n",
        "        if np in june_phrases:\n",
        "            june_phrases[np] += 1\n",
        "        else:\n",
        "            june_phrases[np] = 1\n",
        "\n",
        "#Print the top 10 things she mentioned in june\n",
        "\n",
        "for np in sorted(june_phrases, key=june_phrases.get, reverse=True)[0:10]:\n",
        "    print(np, june_phrases[np])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riY9gZJwrDj0"
      },
      "source": [
        "## Q7\n",
        "\n",
        "Find a URL to perform some analysis. You can try to get something from:\n",
        "- [CBC news](https://www.cbc.ca/news)\n",
        "- [New York Times](https://www.nytimes.com/)\n",
        "- The text of a tweet...\n",
        "- What else?\n",
        "\n",
        "Paste your URL into the variable defined in line 1.\n",
        "\n",
        "Share the URL you've analyzed by sharing a link in the chat box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "V36J0Dn3rDj1"
      },
      "outputs": [],
      "source": [
        "URL = \"https://www.cbc.ca/news/science/wikipedia-bias-1.6129073\"\n",
        "\n",
        "res = requests.get(URL)\n",
        "html_page = res.content\n",
        "soup = BeautifulSoup(html_page, 'html.parser')\n",
        "text = soup.find_all(text=True)\n",
        "\n",
        "output = ''\n",
        "blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head',\n",
        "    'input',\n",
        "    'script',\n",
        "    'style',\n",
        "]\n",
        "\n",
        "for t in text:\n",
        "    if t.parent.name not in blacklist:\n",
        "        output += '{} '.format(t)\n",
        "\n",
        "eTB = TextBlob(output)\n",
        "\n",
        "#Sentiment\n",
        "print(\"Sentiment:\\n\")\n",
        "print(eTB.sentiment)\n",
        "\n",
        "\n",
        "#Noun Phrases\n",
        "print(\"\\nNoun Phrases:\\n\")\n",
        "ex_phrases = dict()\n",
        "\n",
        "#We go through and count how many times a Noun-Phrase shows up\n",
        "for np in eTB.noun_phrases:\n",
        "    if np in ex_phrases:\n",
        "        ex_phrases[np] += 1\n",
        "    else:\n",
        "        ex_phrases[np] = 1\n",
        "\n",
        "#We'll print the noun-phrase and times they show up\n",
        "#We'll stop when the noun-phrases only show up once\n",
        "\n",
        "for np in sorted(ex_phrases, key=ex_phrases.get, reverse=True):\n",
        "    if ex_phrases[np] == 1:\n",
        "        break\n",
        "    print(np, ex_phrases[np])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zp4oAuMrDj1"
      },
      "source": [
        "# And now for something completely different\n",
        "\n",
        "## A very basic classifier\n",
        "\n",
        "We looked at how to score the sentiment of a corpus. We can also create a classifier on our own if we provide testing and training data. In our example we are going to look at whether some statements about Twitter are subjective ( _sub_ ) or objective ( _obj_ )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "u1JbF746rDj1"
      },
      "outputs": [],
      "source": [
        "train = [\n",
        "    ('I think Twitter is stupid', 'sub'),\n",
        "    ('Lots of people send too much time on Twitter.', 'obj'),\n",
        "    ('Twitter is a waste of time.', 'sub'),\n",
        "    ('Twitter can be used to find information.', 'obj'),\n",
        "    ('Many celebrites have Twitter accounts.', 'obj'),\n",
        "    ('I think there is too much misinformation on Twitter', 'sub'),\n",
        "    (\"I don't like Twitter.\", 'sub'),\n",
        "    (\"Twitter is the best ever\", 'sub'),\n",
        "    ('Twitter is great because all of my friends us it', 'sub'),\n",
        "    ('Twitter is a fortune 500 company', 'obj')\n",
        "    ]\n",
        "\n",
        "\n",
        "test = [\n",
        "     ('Twitter is a company', 'obj'),\n",
        "     (\"You can't communicate well with such short sentences\", 'sub'),\n",
        "     (\"Twitter is disruptive to soceity\", 'sub'),\n",
        "     (\"Over 500 million people use Twitter\", 'obj'),\n",
        "     ('A Twitter message can have 280 characters', 'obj'),\n",
        "     (\"A Twitter message is always stupid\", 'sub')\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZrDOdikRrDj1"
      },
      "outputs": [],
      "source": [
        "#Builds the classifer and run the training data through it\n",
        "cl = NaiveBayesClassifier(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wJkcK3tarDj2"
      },
      "outputs": [],
      "source": [
        "#Classify each item in the test set to see how well the classifier works.\n",
        "\n",
        "for item in test:\n",
        "    print(\"Item: \",item[0])\n",
        "    print(\"Guess: \\t\\t\",cl.classify(item[0]))\n",
        "    print(\"Actual: \\t\",item[1],\"\\n\")\n",
        "\n",
        "print(\"\\nAccuracy of guesses\", cl.accuracy(test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5pULMTryrDj2"
      },
      "outputs": [],
      "source": [
        "# We can have the classifer tells us some things it has noticed with the samples\n",
        "cl.show_informative_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRLg7UeerDj2"
      },
      "source": [
        "## Q8\n",
        "\n",
        "As our last activity try to create your own classifier in the next code cell. You'll just need to provide examples for the classifer to train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RgxuzM8erDj2"
      },
      "outputs": [],
      "source": [
        "train_2 = [\n",
        "    ('I love this sandwich.', 'pos'),\n",
        "    ('','pos'), #add a positive sentence\n",
        "    ('','pos'), #add a positive sentence\n",
        "    ('','pos'), #add a positive sentence\n",
        "    ('I do not like this restaurant', 'neg'),\n",
        "    ('','neg'), #add a negative sentence\n",
        "    ('','neg'), #add a negative sentence\n",
        "    ('','neg')  #add a negative sentence\n",
        "    ]\n",
        "\n",
        "\n",
        "cl_2 = NaiveBayesClassifier(train_2)\n",
        "\n",
        "print(\"Our Important features:\")\n",
        "cl_2.show_informative_features()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz_5tg2TrDj2"
      },
      "source": [
        "Run the following cell as often as you'd like to have the classifier attempt more sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AqBckbJWrDj3"
      },
      "outputs": [],
      "source": [
        "print(\"\\nInput a sentence you wish to classify\")\n",
        "test_sentence = input()\n",
        "print(\"Classification category: \", cl_2.classify(test_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5H-KicfrDj3"
      },
      "source": [
        "# Congrats!\n",
        "\n",
        "You have now learned the basics of Text Analysis using Python and TextBlob.\n",
        "\n",
        "We also offer a workshop called [Advanced Text Analysis](https://brockdsl.github.io/Advanced_Text_Analysis_with_Python/) if you'd like to dig into more details on topic modelling.\n",
        "\n",
        "All of our workshops are posted on [Eventbrite](https://brockdsl.eventbrite.com/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCJyQHJQrDj3"
      },
      "source": [
        "# More Links\n",
        "\n",
        "\n",
        "- [Sentiment Analysis of Tweets Using Python](https://www.greycampus.com/blog/data-science/sentiment-analysis-on-twitter-tweets-using-python) - a case study that uses twitter data to generate sentiment values.\n",
        "\n",
        "\n",
        "- [VADER](https://github.com/cjhutto/vaderSentiment#python-demo-and-code-examples) - (Valence Aware Dictionary and sEntiment Reasoner) is a sentiment library designed to be used for social media that can better reflect the sentiment of slang, emoticons and hashtags.\n",
        "\n",
        "\n",
        "- [Topic Modelling with gensim](https://towardsdatascience.com/topic-modeling-with-gensim-a5609cefccc) - The next step in your understanding of text analysis should be topic modelling, where we try to determine what topics are in a corpus. It is bit too complex to tackling in this workshop.\n",
        "\n",
        "\n",
        "- [Kaggle](https://www.kaggle.com/search?q=text+analysis) - If you do data science using a Python in a notebook, this the place for you.\n",
        "\n",
        "\n",
        "- [Python for Librarians](https://libraryjuiceacademy.com/shop/course/270-python-for-librarians/) - An upcoming workshop that will look at many interesting pieces of Python."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}